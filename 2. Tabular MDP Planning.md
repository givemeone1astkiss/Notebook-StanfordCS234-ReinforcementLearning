# **Tabular MDP Planning**

## 1 Value Function

### 1.1 Bellman Equation

Bellman Equation描述了给定策略下或最优策略下的期望累积折扣奖励。方程有两种形式:

* Bellman 期望方程:
  $$V^\pi(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^\pi(s') \right)$$
  $$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) \sum_{a' \in A} \pi(a' \mid s') Q^\pi(s', a')$$
* Bellman 最优方程:
  $$V^*(s) = \max_a \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^*(s') \right)$$
  $$Q^*(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) \max_{a'} Q^*(s', a')$$

Bellman方程以递归的形式考虑未来奖励的折现累积。

### 1.2 MRP

MRP价值函数满足:
$$V(s)=R(s)+\gamma\sum_{s'\in S}P(s'|s)V(s')$$
其中 $R(s)$ 表示即时奖励, $\gamma\sum_{s'\in S}P(s'|s)V(s')$ 表示未来奖励折现的总和。
对于有限状态的MDP, 可以用矩阵形式表示 $V(s)$
$$\begin{pmatrix} V(s_{1}) \\ V(s_{2}) \\ \vdots \\ V(s_{n}) \end{pmatrix}=\begin{pmatrix} R(s_{1}) \\ R(s_{2}) \\ \vdots \\ R(s_{n}) \end{pmatrix} + \gamma \begin{pmatrix}P(s_{1}|s_{1}) & P(s_{2}|s_{1}) & \cdots & P(s_{n}|s_{1}) \\P(s_{1}|s_{2}) & P(s_{2}|s_{2}) & \cdots & P(s_{n}|s_{2})\\\vdots &\vdots & \ddots & \vdots \\ P(s_{1}|s_{n}) & P(s_{2}|s_{n}) & \cdots & P(s_{n}|s_{n})\end{pmatrix} \begin{pmatrix} V(s_{1}) \\ V(s_{2}) \\ \vdots \\ V(s_{n}) \end{pmatrix}$$
$$V=R+\gamma PV$$
对 $V$ 进行求解, 可得:
$$V=(1-\gamma P)^{-1}R$$
使用此方法求解有以下不足:

* 求解逆矩阵需要的时间复杂度为 $O(n^{3})$
* 需要 $1-\gamma P$ 是可逆的

为了避免直接进行解析求解, 使用动态规划, 以迭代的方式求解:

* 对于所有状态 $s$ 初始化 $V_{0}(s)=0$
* 迭代计算:
  $$V_{k}(s)=R(s)+\gamma\sum_{s'\in S}P(s'|s)V_{k-1}(s')$$

对于每一次迭代, 时间复杂度为 $O(|S|^{2})$

### 1.3 MDP

对于MDP, 我们需要额外考虑的一点是智能体的策略:
$$\pi(a|s)=P(a_{t}=a|s_{t}=s)$$
对于一个特定的MDP $(S, R^{\pi}, P^{\pi}, \gamma)$, 有:
$$R^{\pi}(s)=\sum_{a\in A}\pi(a|s)R(s,a)$$
$$P^{\pi}(s^{\pi}|s)=\sum_{a\in A}\pi(a|s)P(s'|s,a)$$
这将MDP和MRP的形式相联系。
对于MDP有类似的迭代算法:

* 对于所有状态 $s$ 初始化 $V_{0}(s)=0$
* 迭代计算: $$V^{\pi}_{k}(s)=\sum_{a}\pi(a|s) \left(R(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^{\pi}_{k-1}(s')\right)$$
  
    对于确定性策略, 有:
    $$V^{\pi}_{k}(s)=R(s,\pi(s))+\gamma\sum_{s'\in S}P(s'|s,\pi(s))V^{\pi}_{k-1}(s')$$

在MDP Control中, 我们不仅要对某个策略进行评估，还要计算出最优的策略:
    $$\pi^{*}(s)={arg}\underset{\pi}{max}V^{\pi}(s)$$
infinite horizon的MDP策略优化有以下性质:

* 确定性(Deterministic): 最优的价值函数和策略是确定的
* 平稳性(Stationary): 最优的决策只依赖于状态而不依赖时间步
* 独特性(Unique): 最优的价值函数是唯一的, 而最优的策略则不一定

## 2 Opimization

### 2.1 Bellman Operator

Bellman operator描述了状态价值函数或动作价值函数的更新规则, 计算结果成为Bellman backup(贝尔曼回溯)。
Bellman operator有两种形式:

* Bellman 期望算子:
  $$(\mathcal{T}^\pi V)(s) = \sum_{a \in A} \pi(a \mid s) \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V(s') \right)$$
  描述了在给定策略下，状态价值函数的更新规则, 是PI的核心规则。。每次应用 Bellman 期望算子，都会根据当前策略计算每个状态的期望累积折扣奖励，并逐步逼近该策略下的真实价值函数 $V^{\pi}$。
* Bellman 最优算子:
  $$(\mathcal{T}^* V)(s) = \max_a \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V(s') \right)$$
  描述了在最优策略下，状态价值函数的更新规则, 是VI的核心规则。每次应用 Bellman 最优算子，都会选择使期望累积折扣奖励最大化的动作，并逐步逼近最优状态价值函数 $V^{*}$。
  当 $\gamma < 1$ 时, Bellman 最优算子是一个收缩算子:
  $$||\mathcal{T}^* V_{1}-\mathcal{T}^* V_{2}||\le c||V_{1}-V_{2}||_{\infty}$$
  证明如下:
  $$
  `\begin{aligned}
  |(\mathcal{T}^* V_1)(s) - (\mathcal{T}^* V_2)(s)| &\leq \max_a \left| \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V_1(s') \right) - \left( R(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V_2(s') \right) \right| \\
  &= \max_a \left| \gamma \sum_{s' \in S} P(s' \mid s, a) (V_1(s') - V_2(s')) \right| \\
  &\leq \gamma \max_a \sum_{s' \in S} P(s' \mid s, a) |V_1(s') - V_2(s')| \\
  &\leq \gamma \max_a \sum_{s' \in S} P(s' \mid s, a) \| V_1 - V_2 \|_\infty \\
  &= \gamma \| V_1 - V_2 \|_\infty
  \end{aligned}
  $$`
  根据 Banach 不动点定理，收缩算子在完备度量空间中有一个唯一的不动点，并且任何初始值通过迭代应用该算子都会收敛到这个不动点。因此，Bellman 算子的收缩性保证了VI的收敛性, 及VI中状态价值函数的初始化并不重要。

### 2.2 Policy Iteration

在策略迭代的过程中逐步进行策略评估和策略改进, 逐步逼近最优策略 $\pi^{*}$。
策略迭代(PI)的工作流程是:

* 对所有状态 $s$, 随机初始化策略 $\pi_{o}(s)$
* 计算当前策略 $\pi_{i}$ 下的状态价值函数 $V^{\pi_{i}}(s)$, 通过求解线性方程组或根据Bellman期望算子迭代得到:
$$V^{\pi}=\mathcal{T}^{\pi}\mathcal{T}^{\pi}\cdots\mathcal{T}^{\pi}V$$
* 根据 $V^{\pi_{i}}(s)$ 更新策略 $\pi_{i}$, 使得每个状态下的动作都最大化动作价值函数 $Q^{\pi_{i}}(s,a)$:
  $$Q^{\pi_{i}}(s,a)=R(s,a)+\gamma\sum_{s'\in S}P(s'|s,a)V^{\pi_{i}}(s')$$
  $$\pi_{i+1}(s)=arg\underset{a}{max}Q^{\pi_{i}}(s,a), \forall s \in S$$

对于一次迭代过程, 有:
$$\underset{a}{max}Q^{\pi_{i}}(s,a)\geq R(s,\pi(s))+\gamma\sum_{s'\in S}P(s'|s,a)V^{\pi_{i}}(s')=V^{\pi_{i}}(s)$$
$$\pi_{i+1}(s)=arg\underset{a}{max}Q^{\pi_{i}}(s,a)$$
这个迭代过程是单调的, 即每一次迭代都会得到一个至少与之前的策略同样好的新策略。
当在一次迭代过程中策略不再改变, 迭代就终止了。

### 2.3 Value Iteration

价值迭代是(VI)采用另一种思路: 在每次迭代中直接更新状态价值函数 $V(s)$, 而不需要显式地维护一个策略, 通过迭代地应用Bellman最优方程逐步逼近最优状态价值函数 $V(s)^{*}$。
价值迭代的工作流程是:

* 初始化状态价值函数 $V(s)$ 为任意值(通常是0)
* 重复以下步骤:
  $$V(s) \leftarrow (\mathcal{T}^* V)(s')$$
  直至收敛:
  $$||V_{k+1}-V_{k}||_{\infty}\le\epsilon$$
* 一旦 $V(s)$ 收敛至最优状态价值函数 $V(s)^{*}$, 就可以导出最优策略:
  $$\pi^{*}(s)=arg\underset{a}{max}Q^{*}(s,a)$$
